---
title: "Inférence démographique - Approximate Bayesian Computation"
author: 
  - Gustavo Magaña López
  - Théo Roncalli
date: "20/12/2021"
output:
  bookdown::pdf_document2:
    includes:  
      in_header: float_adjustement.tex
header-includes:
    - \usepackage{fancyhdr}
    - \addtolength{\headheight}{1.0cm} % make more space for the header
    - \pagestyle{fancyplain} % use fancy for all pages except chapter start
    - \rhead{\includegraphics[height=1.2cm]{/home/gml/Documents/Master/M2_AMI2B/MetaGenomique/rendu_tp2/logo.png}} % right logo
    - \renewcommand{\headrulewidth}{0pt} % remove rule below header
    - \usepackage{float}
    - \floatplacement{figure}{H}

bibliography: [packages.bib] #, articles.bib]
nocite: '@*'
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = F, warning = F, message = F, fig.pos= "h", cache = T
)
.annotation.pkgs <- c(
  "here", "abc", "coala", "magrittr", "stringr",
   "dplyr", "tibble", "foreach", "doRNG", 
   "ggplot2", "RColorBrewer", "gridExtra")

knitr::write_bib(c(
  .annotation.pkgs, 'knitr', 'rmarkdown', 
  'gridExtra', 'grid', 'cowplot', 'magick'
), file = 'packages.bib')

library(here)
library(grid)
library(cowplot)
library(gridExtra)
library(png)
library(magrittr)
library(magick)
library(ggplot2)

source(here("ABC/customfuncs.R"))
```


# Introduction {#intro}

Dans le cas présent, un jeu de données nous est fourni sur le chromosome 22 pour
54 individus, c’est-à-dire pour 108 haploïdes. 
Celui-ci provient du 
[Panel Diversity de Complete Genomics](http://www.completegenomics.com/public-data/). 
Nous avons deux fichiers : le premier renvoie diverses informations sur chaque 
haploïde (numéro du chromosome, id du snp, position sur le chromosome de 
référence, nucléotide de référence, variant, et le nucléotide effectivement 
observé pour chaque haploïde. 
Le second fichier renvoie des métadonnées sur chaque haploïde. 
Nous avons l’identifiant de l’individu, le code renseignant sur la population
à laquelle il appartient et sa région de provenance. 
Parmi les populations recensées, nous en avons plusieurs, comme les 
portoricains (PUR), les Utah d’ascendance européenne du nord et de 
l’ouest (CEU), les Yoruba (YRI), les chinois Han (CHB), etc. 
Les scientifiques considèrent les CEU comme Européens alors que ces 
individus habitent aux Etats-Unis car il y a eu une migration des Européens 
de l’Ouest vers les Amériques, ce qui implique un bottleneck. 
Comme c’est une communauté très fermée, on le considère comme un 
échantillon européen étant donné qu’il n’y a pas d’échange de matériel 
génétique avec d’autres populations américaines.

Afin de reconstruire les tailles de population et histoires démographiques pour l’espèce humaine, nous pouvons commencer par étudier certaines statistiques telles que le D de Tajima:

$$
D = \frac{\Theta_T - \Theta_W}{\sqrt{V(\Theta_T - \Theta_W)}}
$$

La **figure \@ref(fig:dtaj)** fournit le D de Tajima pour chaque population recensée. 
Un D de Tajima positif signifie qu’il y a un excès d’allèles en fréquence 
intermédiaire, alors qu’un D de Tajima négatif signifie qu’il y a 
plutôt de nombreux singletons. 
Dans le cas présent, on remarque que les populations africaines 
(YRI, ASW, LWK, MKK) ont un D de Tajima négatif (très élevé en valeur absolue). 
Nous pouvons donc supposer que ces populations africaines ont connu une expansion démographique.
En revanche, les autres populations ont un D de Tajima positif, 
ce qui laisse supposer qu’il y a eu un bottleneck pour celles-ci. 
On peut donc supposer qu’il y a eu une migration de l’espèce Homo Sapiens hors 
d’Afrique. 
Cette observation déduite de nos résultats semble être confirmée par la théorie 
"Origine africaine de l’Homme moderne". On appelle ce phénomène le 
Out Of Africa (OOA) qui désigne une très petite partie de la population 
africaine ayant migré vers les autres continents il y a environ 
50000 à 60000 années.

```{r dtaj, fig.cap="D de Tajima pour chaque population", out.width='80%', fig.align='center'}
snq1 <- ggdraw() + draw_image(readPNG(here("ABC/Figures/D_tajima.png")))
print(snq1)
```

# Sélection du modèle démographie {#sel_mod_demo}

Au regard des D de Tajima différents que nous avons obtenu précédemment, les populations humaines que nous étudions ne semblent pas avoir les mêmes caractéristiques de diversité génétique. Par conséquent, nous allons estimer des modèles démographiques plus complexes pour élaborer des analyses plus approfondies.

## Hypothèses {#hypo}

D’après les statistiques obtenues précédemment, nous pouvons supposer que les
populations humaines ont subies des processus démographiques différents. 
Ces différences de processus démographiques seraient surtout importantes entre
les populations africaines (YRI, ASW, LWK, MKK) et les autres populations. 
Pour la suite du devoir, nous nous intéresserons à trois scénarios 
démographiques pour chacune des populations étudiées :

  * constant (pas de changement significatif de la taille de la population au cours du temps)
  * bottleneck (population connaissant un déclin démographique important puis, après une période de plusieurs générations, une croissance démographique très importante ramenant soudainement l’effectif de la population à son état initial)
  * expansion (population connaissant une forte croissance démographique à partir d’une certaine période)

**Figure \@ref(fig:schemes)** fournit une représentation graphique dans le temps 
de la taille d'une population et de ses coalescences pour chaque scénario
démographique. 
Pour simuler ces modèles, différents paramètres peuvent être utilisés, 
tels que la taille effective de la population Ne ou le taux de mutation . 
Ces valeurs sont souvent inconnues et nécessite toutefois des simulations.

```{r schemes, fig.cap="Schématisation des modèles démographiques", out.width='80%', fig.align='center'}
snq2 <- ggdraw() + draw_image(readPNG(here("ABC/Figures/schemes.png")))
print(snq2)
```

Notons que le spectre de fréquence allélique SFS fournit des informations 
importantes sur les scénarios démographiques passés. Par exemple, la 
proportion de fréquences alléliques dérivés pour des comptages élevés est 
plus importante pour le scénario de population constante en comparaison 
des deux autres scénarios. Observons également que Le D Tajima est une 
statistique résumant la densité du SFS. 
Le D Tajima espéré pour chaque modèle démographique est le suivant:

  * $D_{Tajima} < 0$ : expansion car excès d’allèles en fréquence intermédiaire (Theta Tajima supérieur au Theta Watterson)
  * $D_{Tajima} = 0$ : constant (population à l’équilibre neutre sous le modèle de Wright-Fisher)
  * $D_{Tajima} > 0$ : bottleneck car pauvreté d’allèles en fréquence intermédiaire (Theta Tajima inférieur au Theta Watterson)

3.2 Construire une base de simulation de référence

Pour retracer l’histoire démographique humaine, le modèle ABC peut être utilisé.
Différentes statistiques résumées peuvent être utilisées : comptage des allèles,
diversité génétique, spectre des fréquences alléliques, etc. 
Ces statistiques résumées peuvent être simulées à partir de différents 
paramètres tels que qu'énoncé précédemment (comme par exemple le taux de mutation).
La distribution à priori de chacun de ces paramètres suivent une loi uniforme. 
Nous pouvons proposer des bornes faibles afin d’être parcimonieux dans nos résultats.
Si nous fixons un taux de tolérance trop élevé, nos trois modèles vont tendre 
vers l’équiprobabilité. En revanche, il ne faut pas fixer un taux de tolérance
trop faible non plus, afin d’éviter d’augmenter la variance de nos résultats qui
seraient trop dépendantes d’une très faible quantité de simulations conservées. 
Un taux de tolérance de $5\%$ semble donc intéressant et raisonnable.

Dans le cas présent, un jeu de statistiques résumées déjà simulées pour 50000 
valeurs différentes de paramètres pour les trois scénarios démographiques est 
déjà proposé. Les statistiques résumées simulées sont le Theta Tajima, 
le D Tajima et la variance du D Tajima. 
Nous rappelons que le D Tajima est une statistique résumée du SFS et permet 
donc de retracer l’évolution démographique d’une population.
**Figure \@ref(fig:tajs)** fournit les boxplot de ces trois statistiques résumées 
pour les trois scénarios démographiques étudiés : 
population constante, goulot d’étranglement
et expansion démographique. On remarque que la médiane du D Tajima est positif
en cas de bottleneck, nul pour une population constante et fortement négative
pour une population en expansion. Ces résultats sont en cohérence avec 
la théorie établie. Également, on remarque que la variance du D Tajima est 
plus élevée en cas de bottleneck et plus faible en cas de population constante
et en expansion.

```{r tajs, fig.cap="Statistiques résumées simulées pour chaque scénario démographique", out.width='80%', fig.align='center'}
snq3 <- ggdraw() + 
  draw_image(readPNG(here("ABC/Figures/Aggregated_boxplot_tajima.png")))
print(snq3)
```

Nous allons maintenant nous intéresser à la sélection de modèle démographique 
avec la méthode Approximate Bayesian Computation (ABC) en récupérant les 
probabilités bayésiennes a posteriori pour chacun des scénarios. 
Pour commencer, nous pouvons récupérer une unique simulation et ses statistiques
résumées. Par exemple, prenons la première simulation associée à un goulot
d’étranglement. 
Pour cette simulation, nous avons $\Theta_{Tajima} = 0.001053111$, 
$D_{Tajima} = 0.05764936$ et sa variance  $Var(D_{Tajima}) = 1.302449.$
Si nous appliquons la méthode ABC avec un seuil de tolérance fixé à $5\%$, 
nous obtenons les probabilités a posteriori suivantes : $13.20\%$ pour une 
population constante, $1.33\%$ pour un goulot d’étranglement et $86.67\%$ pour
une population en expansion. En revanche, si nous avons un seuil de tolérance 
égal à $100%$, nous obtenons des probabilités a posteriori équiprobable, 
c'est-à-dire à $33.3\%$ chacun. Cela résulte du fait que nous conservons 
toutes les simulations lorsque le seuil de tolérance est à $100\%$ et il 
n’est donc pas possible de prédire le scénario démographique auquel elle appartient.
En revanche, lorsque nous fixons un seuil de tolérance à $5\%$, nous prenons
uniquement les $5\%$ de simulations les plus proches de l'observation et nous
utilisons les proportions de chaque scénario simulé récupéré pour estimer la
probabilité bayésienne a posteriori. Ajoutons que nous avons retiré 
l’observation pour estimer son postérieur. Nous faisons cela afin d’avoir une
estimation out-of-sample, et donc corriger le biais de l’erreur d’apprentissage.

Nous avons 3 modèles démographiques. Nous définissons le taux de mal classés comme suit:



$$
MCR = \frac{1}{n} \sum\limits_{i=1}^{n} \mathbf{1}\lbrace y_{i} \neq \hat{y} (x_{i}) \rbrace
$$




Si l’on prédisait un modèle au hasard, le taux de mauvaise classification serait
de deux tiers. Avec le modèle computationnel ABC, nous estimons le scénario
démographique en fonction des autres observations. 
Toutefois, deux hyperparamètres sont requis pour notre modèle ABC :

  * Seuil de tolérance $\varepsilon$
  * Méthode utilisée $\alpha$

Un hyperparamètre est un paramètre sélectionné par l’utilisateur, en amont de
l’exécution du modèle. Lorsqu’un modèle est nouveau, la sélection de 
l’hyperparamètre est arbitraire. Néanmoins, dans le cas de prédiction ou
classification, il est important de procéder à des tests computationnels pour
sélectionner une valeur pour l’hyperparamètre. Dans notre cas par exemple, 
nous avons un problème de classification à trois modalités. Notre objectif est
d’utiliser un modèle qui prédit un scénario démographique en minimisant le taux
d’erreur. Pour évaluer la qualité de prédiction associée à un hyperparamètre,
différentes méthodes sont possibles. Dans le cas présent, nous allons utiliser
une méthode de cross-validation pour sélectionner la valeur des hyperparamètres.
La validation croisée consiste à diviser les données en deux sous 
jeux de données : le premier est appelé jeu de données d’apprentissage 
(servant à calculer les estimateurs pour la prédiction du modèle) et le second
est appelé jeu de données test (servant à calculer l’erreur de prédiction). 
Ainsi, nous chercherons à minimiser l’erreur test comme suit :

$$
\lbrace\hat{\varepsilon}, \hat{\alpha} \rbrace = \arg \min \frac{1}{n} \sum\limits_{i=1}^{n} \mathbf{1}\lbrace y_{i} \neq \hat{y}^{-i} (x_{i}) \rbrace 
$$


Pour rechercher la valeur de nos deux hyperparamètres (seuil de tolérance 
$\varepsilon$ et méthode utilisée $\alpha$), nous utiliserons une méthode basée sur le
_leave-one-out cross validation_. Cette dernière consiste à considérer, 
pour une observation donnée, un échantillon d’apprentissage composé de 
toutes les observations sauf celle donnée, et d’un échantillon test composé
de cette unique observation, puis de calculer le modèle sur l’échantillon
d’apprentissage et la prédiction sur l’observation donnée.
Cette opération est réitérée sur l’ensemble des observations disponibles 
afin que nous puissions calculer le taux de mal classés.

Dans le présent travail, une fonction nous a été fournie pour effectuer le
leave-one-out cross validation. Comme le modèle ABC est consommateur en temps,
l’énoncé nous indique d’utiliser directement la méthode de base, i.e. rejet.
En revanche, nous allons évaluer le taux de mal classés en fonction du seuil
de tolérance epsilon. Le leave-one-out cross validation est une opération 
coûteuse en temps. Afin de pallier ce problème, nous avons parallélisé 
la méthode de cross validation. 
Nous avons donc lancé le (pseudo) leave-one-out cross-validation sur 
150 observations (50 pour chaque scénario) pour différents seuils de tolérance. 
**Figure \@ref(fig:loocv)** reporte le taux d’erreur test pour un seuil de tolérance epsilon allant 
de $0.05\%$ à $10\%$. On remarque que le seuil de tolérance est croissant à 
mesure que le seuil de tolérance augmente. 
Le taux d’erreur tend vers deux tiers à mesure que nous approchons de un. 
Il convient donc de fixer un seuil de tolérance faible pour minimiser le taux
d’erreur. Toutefois, on remarque également du bruit pour des seuils de tolérance
très faibles (entre $0.05\%$ et $2.5\%$). En effet, si notre modèle ne 
prend qu'un très faible nombre de simulations proches de l’observation, 
le poids d’une simulation est très important, ce qui augmente la variance de nos
prédictions. Il convient donc de fixer un seuil de tolérance faible mais 
suffisamment élevé également pour éviter de capturer du bruit. 
Un seuil de tolérance fixé à $2.5\%$ semble raisonnable.

```{r loocv, fig.cap="Taux d’erreur test en leave-one-out cross-validation", out.width='70%', fig.align='center'}
errs1 <- 
  my.read.table(here("ABC/data/cv4postpr_nval_150_tols_0.005_0.1_50.csv"))
plot_cv_rej(errs1) + 
  geom_smooth() +
  labs(x="Tolerance", y="% Misclassified")
```

## Sélection du modèle démographique pour les données réelles {#real-modsel}

Maintenant que nous avons déterminé la valeur de nos hyperparamètres grâce au
cross-validation sur données simulées 
(seuil de tolérance $\varepsilon = 2.5\%$ et méthode utilisée 
$\alpha = \text{rejetction}$), 
nous pouvons appliquer le modèle ABC sur nos données réelles. 
TODO : reference table
Table \@ref(tab:likeliest) fournit, pour chaque population, la probabilité bayésienne a posteriori
de chaque scénario démographique et la prédiction du scénario. 
Nous remarquons que pour quatre populations, le scénario d’expansion est prédit.
Cela concerne les quatre populations africaines (YRI, ASW, LWK, MKK). Parmi les
populations pour lesquelles le modèle ABC prédit un goulot d’étranglement,
nous avons les résidents de l'Utah d'ascendance européenne du Nord et de l'Ouest
(CEU), les chinois Han à Pékin (CHB), les japonais à Tokyo (JPT) et les Toscans
en Italie (TSI). Pour toutes les autres populations, le modèle ABC prédit une
démographique constante dans le temps. Pour certaines populations, le scénario
démographique prédit est très vraisemblable. C’est le cas par exemple pour les
Yoruba à Ibadan au Nigéria (YRI) pour lesquels nous estimons le scénario
d’expansion à $93.3\%$ ou les individus d’ascendance africaine dans le sud-ouest
des États-Unis (ASW) pour lesquels nous estimons le même scénario à $90.7\%$.
Pour d’autres populations, le scénario démographique prédit est plus incertain.
Par exemple, les portoricains (PUR), les individus d’ascendance mexicaine à 
Los Angeles (MXL) et les indiens Gujarati à Houston au Texas (GIH) ont une
probabilité d’avoir une population constante dans le temps à 55% environ et
d’avoir connu un goulot d’étranglement à 45% environ.
Nous pouvons donc nous demander si le scénario de prédiction pour ces 
populations est correct. Si, pour ces populations, le scénario de goulot
d’étranglement était vrai, alors la théorie de l’origine africaine de l’Homme 
moderne se confirmerait davantage, avec un scénario d’expansion pour les 
populations africaines et un scénario de bottleneck pour toutes les autres
populations.

```{r likeliest}
likliest <- my.read.table(here("ABC/data/likeliest.csv"))
likliest %>% 
  as.data.frame() %>% 
  dplyr::mutate_if(is.numeric, dplyr::funs(as.character(signif(., 3)))) %>% 
  knitr::kable(caption="Prédiction du scénario démographique pour chaque population")
```

Pour vérifier le goodness-of-fit de nos résultats, nous pouvons utiliser l’ACP.
Figure \@ref(fig:gofacp) fournit les ACP pour les populations YRI et PUR.
Nous remarquons que pour les Yoruba à Ibadan au Nigéria (YRI), le scénario
d’expansion est bien prédit. En revanche, pour les portoricains (PUR), nous
remarquons qu’il est plus difficile de prédire correctement le scénario entre
population constante et bottleneck (comme nous l’avons souligné précédemment). 
En effet, l’observation se situe à l’intérieur du cercle pour la population
constante et du cercle pour le bottleneck.


```{r gofacp, fig.cap="ACP des statistiques résumées (populations YRI et PUR)"}
yri <- ggdraw() + draw_image(readPNG(here("ABC/Figures/yri.png")))
pur <- ggdraw() + draw_image(readPNG(here("ABC/Figures/pur.png")))

grid.arrange(yri, pur, ncol=2, nrow=1)
```

# Estimer les paramètres d'un modèle donné


Ayant choisi des modèles démographiques maintenant nous nous intéressons à 
l'estimation des paramètres utilisés précédemment. Ce sont les suivants :

 * $N_e$, la taille efficace avant et après le goulot d’étranglement.
 * $a$, le facteur de réduction, i.e. $N_e$ divisée par la taille pendant le goulot d’étranglement ($a = N_e / N_{bottleneck}$).
 * $duration$, la durée du goulot (en années).
 * $start$, temps depuis le début du goulot (en années).

## Exploration, intuitions

Toute analyse bayésienne repose sur les mêmes principes : la formulation d'un 
modèle, ajuster le modèle aux données pour finalement améliorer le modèle
en vérifiant la qualité de l'ajustement et en le comparant à d'autres modèles.

Le Théorème de Bayes permet de calculer le posterior $P(\theta|X)$ en fonction
de la vraisemblance $P(X |\theta)$, le prior $P(\theta)$ et l'évidence $P(X)$
fournie par nos données. Ce théorème peut être vu comme une façon de "mettre à
jour nous croyances" sur la valeur de notre paramètre d'intérêt $\theta$ au fur
et à mesure que l'on obtient plus d'informations sur le phénomène étudié $P(X)$.

$$
 P(\theta| X) = \frac{P(X |\theta)P(\theta)}{P(X)}
$$

En sachant que les paramètres sont tirés indépendamment, 
à partir du théorème, on fait la substitution de 
$\theta = (N_e, a, duration, start)$ dans le prior, ce qui nous permet 
de calculer le posterior:

$$
 P(\theta| X) = 
 \frac{P(X |\theta)\prod\limits_{i=1}^{n}\theta_i}{P(X)} =
 \frac{P(X |\theta)P(N_e)P(a)P(duration)P(start)}{P(X)} 
$$


Les distributions à priori des paramètres de la **figure \@ref(fig:histss)** 
sont toutes uniformes sauf une, celle
de $a$, le facteur de réduction. Cela s'explique par l'interprétation de chacun 
des paramètres dans le contexte de l'inférence démographique. A priori, nous
ne savons pas quelle aurait pu être la taille efficace ($N_e$) avant un boulot
d'étranglement. Le même raisonnement s'applique à la durée du goulot 
($duration$) et le temps depuis le goulot ($start$). Pour cette raison l'hypothèse
la plus pertinente est celle d'une équiprobabilité entre les valeurs minimum
et maximum que l'on peut imaginer. Or, quant au facteur de réduction, 
si un sous-ensemble trop réduit migre ou survie à une catastrophe naturelle 
($min(N_{bott})\rightarrow max(a)$ grand) la probabilité de survie et puis de reproduction
devient trop faible. Cela justifie la loi avec une densité plus importante 
pour faibles valeurs de $a$ que l'on observe.

```{r data.manage, echo=F, include=F, warning=F, message=F, cache=T}
stat.1000g <- my.read.table(here("ABC/data/stat1000g.csv"))
loaded <- load(file = here("ABC/simu.expeA.RData"))

sumstats.bott <- subset(sumstats.expeA, subset=models.expeA=="bott")
params.bott.tb <- tibble(params.bott)
suppressMessages({
  .ne.hist <- params.bott.tb %>% ggplot(aes(x=Ne)) + geom_histogram(colour="black", fill="white")
  .a.hist <- params.bott.tb %>% ggplot(aes(x=a)) + geom_histogram(colour="black", fill="white")
  .dur.hist <- params.bott.tb %>% ggplot(aes(x=duration)) + geom_histogram(colour="black", fill="white")
  .star.hist <- params.bott.tb %>% ggplot(aes(x=start)) + geom_histogram(colour="black", fill="white")
  .param.hists <- grid.arrange(
    .ne.hist, .a.hist, .dur.hist, .star.hist, 
    nrow=2, ncol=2#,
    #top=textGrob("")
  )
})
```


```{r histss, out.width='70%', fig.align='center', fig.cap="Distribution empirique des paramètres"}
plot(.param.hists)
```

En analysant le comportement des statistiques résumées en fonction de la taille
efficace, nous constatons que $\theta_{Tajima}$ ($pi$) montre une claire corrélation
positive. Le $D_{Tajima}$ ainsi que sa variance montrent d'abord une corrélation 
négative et puis positive dans la région $N_e \leq 1000$. Après les deux statistiques
semblent atteindre un plateau.

```{r trends, out.width='70%', fig.align='center', fig.cap="Statistiques vs Ne", fig.pos="h"}
trends <- ggdraw() + draw_image(readPNG(here("ABC/Figures/trends2.png")))
trends
```

\newpage
## Exemple d’inférence

Afin de voir comment la fonction `abc` nous permet d'échantillonner la loi à
posteriori de nos paramètres ce qui souvent ne peut pas se faire de façon analytique.
La **figure \@ref(fig:lois)** montre l'échantillonnage du posterior de chacun de 
nos paramètres d'intérêt pour une observation tirée aléatoirement. 
La fonction `abc` a été appelée en fixant `tol=0.05` et `method='loclinear'`.
Nous pouvons constater que le comportement des trois lois est bien différent 
pour chacun de nos quatre paramètres. 

```{r lois, out.width='95%', fig.align='center', fig.cap="Comparaison des lois pour chaque paramètre", fig.pos="h"}
lois <- ggdraw() + draw_image(readPNG(here("ABC/Figures/lois2.png")))
lois
```

Nous observons que $N_e$ est près d'être un cas idéal d'estimation du posterior. 
En partant d'une loi uniforme la méthode de rejet nous permet d'arriver à 
une loi qui s'approche d'une gaussienne qui serait centrée sur la vraie valeur 
de la taille effective de la population.
En appliquant la correction par régression linéaire locale, notre estimation
devient beaucoup plus précise ce qui est mis en évidence par la courbe bleue. 

La qualité des estimations du posterior des autres paramètres est loin d'être
idéal. Pour `start`, les lois du posterior naïf (rejet) et posterior corrigé (rég. lin.)
se superposent avec le prior. Quant à `duration`, les posteriors se sont écartés 
du prior mais on n'a pas une estimation très confiable. Un résultat intéressant 
est celui du paramètre `a` pour lequel l'estimation n'est pas optimale mais 
on voit que le prior et posterieur sont contrairement biaisés. Ceci semble contredire
l'hypothèse que nous avions précédemment émise. Cependant le résultat n'est pas 
conclusif car l'estimation pourrait encore être améliorée et elle n'a été faite que 
sur une observation tirée aléatoirement. De plus, les hyperparamètres 
utilisés ont été choisis arbitrairement et non pas par validation croisée.


\newpage
## Performances

Pour une (ou plusieurs) variable continue, plusieurs métriques existent. L'une 
des plus fréquentes est la racine de l'erreur quadratique moyenne appelée
$RMSE$ pour son nom en anglais. Elle est définie comme suit :

$$
RMSE = \sqrt{ \frac{\sum\limits_{i=1}^{n}(\hat{y}_t - y_{t})^2}{n} }
$$

Cette métrique est facilement utilisable pour comparer les performances de différents
modèles. Elle a l'avantage de se trouver dans le même ordre de grandeur que la variable
d'intérêt. Cependant, la question est : Que faire quand plusieurs paramètres 
doivent être considérés ? Nous pourrions ne regarder qu'une parmi les quatre 
valeurs de RMSE. Nous pourrions attribuer des poids à chaque variable
pour faire une moyenne pondérée. Le choix de poids est crucial car il déterminera
le classement de méthodes de correction et valeurs de tolérance, et donc tous
les résultats obtenus.

La pondération peut être justifiée par :

  * Une hypothèse biologique
  * Un critère numérique pertinent

C'est

```{r rmse, out.width='95%', fig.align='center', fig.cap="rmses", fig.pos="h"}
lois <- ggdraw() + draw_image(readPNG(here("ABC/Figures/rmses.png")))
lois
```


## Application aux données réelles



what

\newpage

# Références et Annexe {-#refs}

<div id="refs"></div>

### Repo github {-#smth} 

L'intégralité du code utilisé pour cet analyse est disponible dans le lien
suivant : [https://github.com/gmagannaDevelop/MetaGenomique/tree/ABC](https://github.com/gmagannaDevelop/MetaGenomique/tree/ABC)

bPeaks 






